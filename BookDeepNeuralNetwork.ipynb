{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "import codecs\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "class HyperParameters:\n",
    "    def __init__(self):\n",
    "        self.num_epochs = 500\n",
    "        self.batch_size = 512\n",
    "        self.rnn_size = 512\n",
    "        self.num_layers = 3\n",
    "        self.keep_prob = 0.7\n",
    "        self.embed_dim = 512\n",
    "        self.seq_length = 30\n",
    "        self.learning_rate = 0.001\n",
    "        self.save_dir = './save'\n",
    "        \n",
    "'''\n",
    "num_epochs = 10000\n",
    "batch_size = 512\n",
    "rnn_size = 512\n",
    "num_layers = 3\n",
    "keep_prob = 0.7\n",
    "embed_dim = 512\n",
    "seq_length = 30\n",
    "learning_rate = 0.001\n",
    "save_dir = './save'\n",
    "'''\n",
    "\n",
    "hp = HyperParameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lookup_tables(text):\n",
    "    \"\"\"\n",
    "    Create lookup tables for the vocabulary\n",
    "    \"\"\"\n",
    "    #Convert the text data to a set, ordering and removing all duplicates\n",
    "    vocab = set(text)\n",
    "    #Creating a dict of set integer to word\n",
    "    int_to_vocab = {key: word for key, word in enumerate(vocab)}\n",
    "    #Creating a dict of word to set integer\n",
    "    vocab_to_int = {word: key for key, word in enumerate(vocab)}\n",
    "    return vocab_to_int, int_to_vocab\n",
    "\n",
    "def token_lookup():\n",
    "    \"\"\"\n",
    "    Create a dict of token replacements\n",
    "    \"\"\"\n",
    "    return {\n",
    "        '.': '||period||',\n",
    "        ',': '||comma||',\n",
    "        '\"': '||quotes||',\n",
    "        ';': '||semicolon||',\n",
    "        '!': '||exclamation-mark||',\n",
    "        '?': '||question-mark||',\n",
    "        '(': '||left-parentheses||',\n",
    "        ')': '||right-parentheses||',\n",
    "        '--': '||emm-dash||',\n",
    "        '\\n': '||return||'\n",
    "    }\n",
    "\n",
    "def get_batches(int_text,hp):\n",
    "    words_per_batch = hp.batch_size * hp.seq_length\n",
    "    num_batches = len(int_text)//words_per_batch\n",
    "    int_text = int_text[:num_batches*words_per_batch]\n",
    "    print(\"Words Per Batch: {0}, Num_Batchs : {1}\".format(words_per_batch, num_batches))\n",
    "    y = np.array(int_text[1:] + [int_text[0]])\n",
    "    print(y)\n",
    "    x = np.array(int_text)\n",
    "    print(x)\n",
    "    print(x.reshape(hp.batch_size, -1))\n",
    "    print(y.reshape(hp.batch_size, -1))\n",
    "    x_batches = np.split(x.reshape(hp.batch_size, -1), num_batches, axis=1)\n",
    "    y_batches = np.split(y.reshape(hp.batch_size, -1), num_batches, axis=1)\n",
    "    \n",
    "    batch_data = list(zip(x_batches, y_batches))\n",
    "    return np.array(batch_data)\n",
    "\n",
    "\n",
    "def pick_word(probabilities, int_to_vocab):\n",
    "    \"\"\"\n",
    "    Pick the next word with some randomness\n",
    "    :param probabilities: Probabilites of the next word\n",
    "    :param int_to_vocab: Dictionary of word ids as the keys and words as the values\n",
    "    :return: String of the predicted word\n",
    "    \"\"\"\n",
    "    return np.random.choice(list(int_to_vocab.values()), 1, p=probabilities)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 13 books\n",
      "Reading Data/DorianGrey.txt\n",
      "Reading Data/GreatExpectations.txt\n",
      "Reading Data/GrimsFairyTales.txt\n",
      "Reading Data/Metamorphasis.txt\n",
      "Reading Data/MobyDic.txt\n",
      "Reading Data/ScarletLetter.txt\n",
      "Reading Data/SherlockHolmes.txt\n",
      "Reading Data/TaleOfTwoCities.txt\n",
      "Reading Data/WarAndPeace.txt\n",
      "Reading Data/aliceinwonderland.txt\n",
      "Reading Data/dracula.txt\n",
      "Reading Data/frankenstein.txt\n",
      "Reading Data/heartofdarkness.txt\n",
      "corpus is 9419385 characters long\n"
     ]
    }
   ],
   "source": [
    "book_filenames = sorted(glob.glob(\"Data/*.txt\"))\n",
    "print(\"found {} books\".format(len(book_filenames)))\n",
    "\n",
    "corpus_raw = u\"\"\n",
    "for filename in book_filenames:\n",
    "    print(\"Reading {}\".format(filename))\n",
    "    with codecs.open(filename,'r', encoding=\"utf-8\", errors='ignore') as book_file:\n",
    "        corpus_raw += book_file.read()\n",
    "        \n",
    "print(\"corpus is {} characters long\".format(len(corpus_raw)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_dict = token_lookup()\n",
    "for token, replacement in token_dict.items():\n",
    "    corpus_raw = corpus_raw.replace(token, ' {} '.format(replacement))\n",
    "corpus_raw = corpus_raw.lower()\n",
    "corpus_raw = corpus_raw.split()\n",
    "\n",
    "vocab_to_int, int_to_vocab = create_lookup_tables(corpus_raw)\n",
    "corpus_int = [vocab_to_int[word] for word in corpus_raw]\n",
    "#serialize the data into the preprocess pickle object\n",
    "pickle.dump((corpus_int, vocab_to_int, int_to_vocab, token_dict), open('preprocess.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():   \n",
    "    input_text = tf.placeholder(tf.int32, [None, None], name='input')\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
    "    lr = tf.placeholder(tf.float32, name='learning_rate')\n",
    "    \n",
    "    vocab_size = len(int_to_vocab)\n",
    "    input_text_shape = tf.shape(input_text)\n",
    "    \n",
    "    lstm = tf.nn.rnn_cell.LSTMCell(num_units=hp.rnn_size)\n",
    "    drop_cell = tf.nn.rnn_cell.DropoutWrapper(lstm,output_keep_prob=hp.keep_prob)\n",
    "    cell = tf.nn.rnn_cell.MultiRNNCell([drop_cell] * hp.num_layers)\n",
    "    \n",
    "    initial_state = cell.zero_state(input_text_shape[0], tf.float32)\n",
    "    initial_state = tf.identity(initial_state, name=\"initial_state\")\n",
    "    \n",
    "    embed = tf.contrib.layers.embed_sequence(input_text, vocab_size, hp.embed_dim)\n",
    "    \n",
    "    outputs, final_state = tf.nn.dynamic_rnn(cell, embed, dtype=tf.float32)\n",
    "    final_state = tf.identity(final_state, name=\"final_state\")\n",
    "    logits = tf.contrib.layers.fully_connected(outputs, vocab_size)\n",
    "    probs = tf.nn.softmax(logits, name=\"probs\")\n",
    "    \n",
    "    cost = tf.contrib.seq2seq.sequence_loss(logits, targets, tf.ones([input_text_shape[0], input_text_shape[1]]))\n",
    "    optimizer = tf.train.AdamOptimizer(hp.learning_rate)\n",
    "    gradients = optimizer.compute_gradients(cost)\n",
    "    capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n",
    "    train_op = optimizer.apply_gradients(capped_gradients)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words Per Batch: 15360, Num_Batchs : 139\n",
      "[ 1170 10538 42994 ... 17834 42451 27810]\n",
      "[27810  1170 10538 ...  2203 17834 42451]\n",
      "[[27810  1170 10538 ... 45240 32279  1170]\n",
      " [ 1170 32279  2524 ... 32279 27321 28196]\n",
      " [32279 20475 13531 ...  7444 17207  2828]\n",
      " ...\n",
      " [11963 35339 11801 ... 28805 20565  8892]\n",
      " [10333 38802 35564 ... 27957 45240 43547]\n",
      " [20565 10646 21518 ...  2203 17834 42451]]\n",
      "[[ 1170 10538 42994 ... 32279  1170  1170]\n",
      " [32279  2524 37038 ... 27321 28196 32279]\n",
      " [20475 13531 37282 ... 17207  2828 38195]\n",
      " ...\n",
      " [35339 11801 47610 ... 20565  8892 10333]\n",
      " [38802 35564 20191 ... 45240 43547 20565]\n",
      " [10646 21518  1170 ... 17834 42451 27810]]\n",
      "Epoch   1 Batch  139/139   train_loss = 6.581   time_elapsed = 271.130   time_remaining = 135294\n",
      "Model Trained and Saved\n",
      "Epoch   2 Batch  139/139   train_loss = 6.577   time_elapsed = 550.460   time_remaining = 137064\n",
      "Epoch   3 Batch  139/139   train_loss = 6.571   time_elapsed = 822.653   time_remaining = 136286\n",
      "Epoch   4 Batch  139/139   train_loss = 6.570   time_elapsed = 1093.973   time_remaining = 135653\n",
      "Epoch   5 Batch  139/139   train_loss = 6.572   time_elapsed = 1365.012   time_remaining = 135136\n",
      "Epoch   6 Batch  139/139   train_loss = 6.569   time_elapsed = 1637.351   time_remaining = 134809\n",
      "Epoch   7 Batch  139/139   train_loss = 6.568   time_elapsed = 1908.676   time_remaining = 134425\n",
      "Epoch   8 Batch  139/139   train_loss = 6.569   time_elapsed = 2179.783   time_remaining = 134057\n",
      "Epoch   9 Batch  139/139   train_loss = 6.569   time_elapsed = 2452.442   time_remaining = 133794\n",
      "Epoch  10 Batch  139/139   train_loss = 6.569   time_elapsed = 2724.189   time_remaining = 133485\n",
      "Epoch  11 Batch  139/139   train_loss = 6.567   time_elapsed = 2995.163   time_remaining = 133149\n",
      "Model Trained and Saved\n",
      "Epoch  12 Batch  139/139   train_loss = 6.570   time_elapsed = 3273.210   time_remaining = 133111\n",
      "Epoch  13 Batch  139/139   train_loss = 6.568   time_elapsed = 3544.867   time_remaining = 132796\n",
      "Epoch  14 Batch  139/139   train_loss = 6.567   time_elapsed = 3816.704   time_remaining = 132494\n",
      "Epoch  15 Batch  139/139   train_loss = 6.566   time_elapsed = 4087.119   time_remaining = 132150\n",
      "Epoch  16 Batch  139/139   train_loss = 6.565   time_elapsed = 4358.797   time_remaining = 131854\n",
      "Epoch  17 Batch  139/139   train_loss = 6.565   time_elapsed = 4629.195   time_remaining = 131524\n",
      "Epoch  18 Batch  139/139   train_loss = 6.566   time_elapsed = 4900.756   time_remaining = 131231\n",
      "Epoch  19 Batch  139/139   train_loss = 6.565   time_elapsed = 5173.822   time_remaining = 130979\n",
      "Epoch  20 Batch  139/139   train_loss = 6.565   time_elapsed = 5437.474   time_remaining = 130499\n",
      "Epoch  21 Batch  139/139   train_loss = 6.565   time_elapsed = 5701.909   time_remaining = 130058\n",
      "Model Trained and Saved\n",
      "Epoch  22 Batch  139/139   train_loss = 6.565   time_elapsed = 5979.016   time_remaining = 129908\n",
      "Epoch  23 Batch  139/139   train_loss = 6.564   time_elapsed = 6249.949   time_remaining = 129618\n",
      "Epoch  24 Batch  139/139   train_loss = 6.560   time_elapsed = 6521.372   time_remaining = 129341\n",
      "Epoch  25 Batch  139/139   train_loss = 6.328   time_elapsed = 6791.011   time_remaining = 129029\n",
      "Epoch  26 Batch  139/139   train_loss = 6.061   time_elapsed = 7061.051   time_remaining = 128728\n",
      "Epoch  27 Batch  139/139   train_loss = 5.837   time_elapsed = 7330.852   time_remaining = 128426\n",
      "Epoch  28 Batch  139/139   train_loss = 5.660   time_elapsed = 7601.425   time_remaining = 128138\n",
      "Epoch  29 Batch  139/139   train_loss = 5.540   time_elapsed = 7869.954   time_remaining = 127819\n",
      "Epoch  30 Batch  139/139   train_loss = 5.464   time_elapsed = 8138.859   time_remaining = 127509\n",
      "Epoch  31 Batch  139/139   train_loss = 5.404   time_elapsed = 8408.812   time_remaining = 127217\n",
      "Model Trained and Saved\n",
      "Epoch  32 Batch  139/139   train_loss = 5.349   time_elapsed = 8684.598   time_remaining = 127012\n",
      "Epoch  33 Batch  139/139   train_loss = 5.301   time_elapsed = 8954.704   time_remaining = 126723\n",
      "Epoch  34 Batch  139/139   train_loss = 5.255   time_elapsed = 9223.880   time_remaining = 126421\n",
      "Epoch  35 Batch  139/139   train_loss = 5.220   time_elapsed = 9493.629   time_remaining = 126130\n",
      "Epoch  36 Batch  139/139   train_loss = 5.192   time_elapsed = 9764.877   time_remaining = 125858\n",
      "Epoch  37 Batch  139/139   train_loss = 5.153   time_elapsed = 10034.292   time_remaining = 125564\n",
      "Epoch  38 Batch  139/139   train_loss = 5.140   time_elapsed = 10305.664   time_remaining = 125295\n",
      "Epoch  39 Batch  139/139   train_loss = 5.120   time_elapsed = 10575.426   time_remaining = 125007\n",
      "Epoch  40 Batch  139/139   train_loss = 5.090   time_elapsed = 10845.592   time_remaining = 124724\n",
      "Epoch  41 Batch  139/139   train_loss = 5.057   time_elapsed = 11115.866   time_remaining = 124443\n",
      "Model Trained and Saved\n",
      "Epoch  42 Batch  139/139   train_loss = 5.025   time_elapsed = 11392.780   time_remaining = 124236\n",
      "Epoch  43 Batch  139/139   train_loss = 5.002   time_elapsed = 11661.924   time_remaining = 123942\n",
      "Epoch  44 Batch  139/139   train_loss = 4.967   time_elapsed = 11931.264   time_remaining = 123651\n",
      "Epoch  45 Batch  139/139   train_loss = 4.957   time_elapsed = 12202.500   time_remaining = 123381\n",
      "Epoch  46 Batch  139/139   train_loss = 4.950   time_elapsed = 12465.949   time_remaining = 123033\n",
      "Epoch  47 Batch  139/139   train_loss = 4.922   time_elapsed = 12730.442   time_remaining = 122700\n",
      "Epoch  48 Batch  139/139   train_loss = 4.878   time_elapsed = 13001.880   time_remaining = 122434\n",
      "Epoch  49 Batch  139/139   train_loss = 4.859   time_elapsed = 13271.461   time_remaining = 122152\n",
      "Epoch  50 Batch  139/139   train_loss = 4.840   time_elapsed = 13543.171   time_remaining = 121889\n",
      "Epoch  51 Batch  139/139   train_loss = 4.814   time_elapsed = 13813.747   time_remaining = 121615\n",
      "Model Trained and Saved\n",
      "Epoch  52 Batch  139/139   train_loss = 4.810   time_elapsed = 14090.573   time_remaining = 121396\n",
      "Epoch  53 Batch  139/139   train_loss = 4.789   time_elapsed = 14360.193   time_remaining = 121113\n",
      "Epoch  54 Batch  139/139   train_loss = 4.779   time_elapsed = 14630.189   time_remaining = 120835\n",
      "Epoch  55 Batch  139/139   train_loss = 4.743   time_elapsed = 14899.403   time_remaining = 120550\n",
      "Epoch  56 Batch  139/139   train_loss = 4.745   time_elapsed = 15169.631   time_remaining = 120274\n",
      "Epoch  57 Batch  139/139   train_loss = 4.714   time_elapsed = 15439.852   time_remaining = 119997\n",
      "Epoch  58 Batch  139/139   train_loss = 4.714   time_elapsed = 15709.186   time_remaining = 119715\n",
      "Epoch  59 Batch  139/139   train_loss = 4.702   time_elapsed = 15978.355   time_remaining = 119431\n",
      "Epoch  60 Batch  139/139   train_loss = 4.684   time_elapsed = 16247.924   time_remaining = 119151\n",
      "Epoch  61 Batch  139/139   train_loss = 4.660   time_elapsed = 16518.338   time_remaining = 118878\n",
      "Model Trained and Saved\n",
      "Epoch  62 Batch  139/139   train_loss = 4.647   time_elapsed = 16795.414   time_remaining = 118651\n",
      "Epoch  63 Batch  139/139   train_loss = 4.629   time_elapsed = 17065.514   time_remaining = 118375\n",
      "Epoch  64 Batch  139/139   train_loss = 4.671   time_elapsed = 17335.528   time_remaining = 118098\n",
      "Epoch  65 Batch  139/139   train_loss = 4.613   time_elapsed = 17606.018   time_remaining = 117825\n",
      "Epoch  66 Batch  139/139   train_loss = 4.580   time_elapsed = 17875.233   time_remaining = 117543\n",
      "Epoch  67 Batch  139/139   train_loss = 4.548   time_elapsed = 18144.869   time_remaining = 117265\n",
      "Epoch  68 Batch  139/139   train_loss = 4.551   time_elapsed = 18414.079   time_remaining = 116984\n",
      "Epoch  69 Batch  139/139   train_loss = 4.528   time_elapsed = 18683.040   time_remaining = 116701\n",
      "Epoch  70 Batch  139/139   train_loss = 4.513   time_elapsed = 18951.907   time_remaining = 116419\n",
      "Epoch  71 Batch  139/139   train_loss = 4.511   time_elapsed = 19221.820   time_remaining = 116143\n",
      "Model Trained and Saved\n",
      "Epoch  72 Batch  139/139   train_loss = 4.506   time_elapsed = 19490.856   time_remaining = 115862\n",
      "Epoch  73 Batch  139/139   train_loss = 4.480   time_elapsed = 19754.751   time_remaining = 115552\n",
      "Epoch  74 Batch  139/139   train_loss = 4.453   time_elapsed = 20023.988   time_remaining = 115273\n",
      "Epoch  75 Batch  139/139   train_loss = 4.432   time_elapsed = 20293.148   time_remaining = 114995\n",
      "Epoch  76 Batch  139/139   train_loss = 4.428   time_elapsed = 20562.082   time_remaining = 114715\n",
      "Epoch  77 Batch  139/139   train_loss = 4.407   time_elapsed = 20831.300   time_remaining = 114437\n",
      "Epoch  78 Batch  139/139   train_loss = 4.407   time_elapsed = 21101.294   time_remaining = 114163\n",
      "Epoch  79 Batch  139/139   train_loss = 4.405   time_elapsed = 21371.133   time_remaining = 113889\n",
      "Epoch  80 Batch  139/139   train_loss = 4.419   time_elapsed = 21640.310   time_remaining = 113612\n",
      "Epoch  81 Batch  139/139   train_loss = 4.403   time_elapsed = 21909.429   time_remaining = 113334\n",
      "Model Trained and Saved\n",
      "Epoch  82 Batch  139/139   train_loss = 4.407   time_elapsed = 22185.486   time_remaining = 113092\n",
      "Epoch  83 Batch  139/139   train_loss = 4.376   time_elapsed = 22454.240   time_remaining = 112812\n",
      "Epoch  84 Batch  139/139   train_loss = 4.341   time_elapsed = 22723.585   time_remaining = 112536\n",
      "Epoch  85 Batch  139/139   train_loss = 4.320   time_elapsed = 22992.464   time_remaining = 112257\n",
      "Epoch  86 Batch  139/139   train_loss = 4.302   time_elapsed = 23262.549   time_remaining = 111985\n",
      "Epoch  87 Batch  139/139   train_loss = 4.297   time_elapsed = 23532.511   time_remaining = 111712\n",
      "Epoch  88 Batch  139/139   train_loss = 4.295   time_elapsed = 23802.658   time_remaining = 111440\n",
      "Epoch  89 Batch  139/139   train_loss = 4.299   time_elapsed = 24071.409   time_remaining = 111161\n",
      "Epoch  90 Batch  139/139   train_loss = 4.287   time_elapsed = 24341.251   time_remaining = 110888\n",
      "Epoch  91 Batch  139/139   train_loss = 4.260   time_elapsed = 24610.645   time_remaining = 110613\n",
      "Model Trained and Saved\n",
      "Epoch  92 Batch  139/139   train_loss = 4.248   time_elapsed = 24885.018   time_remaining = 110360\n",
      "Epoch  93 Batch  139/139   train_loss = 4.229   time_elapsed = 25153.744   time_remaining = 110081\n",
      "Epoch  94 Batch  139/139   train_loss = 4.255   time_elapsed = 25422.332   time_remaining = 109803\n",
      "Epoch  95 Batch  139/139   train_loss = 4.263   time_elapsed = 25691.171   time_remaining = 109526\n",
      "Epoch  96 Batch  139/139   train_loss = 4.255   time_elapsed = 25959.850   time_remaining = 109248\n",
      "Epoch  97 Batch  139/139   train_loss = 4.242   time_elapsed = 26229.570   time_remaining = 108974\n",
      "Epoch  98 Batch  139/139   train_loss = 4.229   time_elapsed = 26491.044   time_remaining = 108667\n",
      "Epoch  99 Batch  139/139   train_loss = 4.210   time_elapsed = 26755.104   time_remaining = 108372\n",
      "Epoch 100 Batch  139/139   train_loss = 4.188   time_elapsed = 27023.741   time_remaining = 108095\n",
      "Epoch 101 Batch  139/139   train_loss = 4.175   time_elapsed = 27292.781   time_remaining = 107820\n",
      "Model Trained and Saved\n",
      "Epoch 102 Batch  139/139   train_loss = 4.167   time_elapsed = 27567.050   time_remaining = 107566\n",
      "Epoch 103 Batch  139/139   train_loss = 4.158   time_elapsed = 27836.126   time_remaining = 107291\n",
      "Epoch 104 Batch  139/139   train_loss = 4.130   time_elapsed = 28105.491   time_remaining = 107017\n",
      "Epoch 105 Batch  139/139   train_loss = 4.135   time_elapsed = 28373.536   time_remaining = 106739\n",
      "Epoch 106 Batch  139/139   train_loss = 4.139   time_elapsed = 28643.757   time_remaining = 106468\n",
      "Epoch 107 Batch  139/139   train_loss = 4.149   time_elapsed = 28912.159   time_remaining = 106191\n",
      "Epoch 108 Batch  139/139   train_loss = 4.142   time_elapsed = 29181.207   time_remaining = 105917\n",
      "Epoch 109 Batch  139/139   train_loss = 4.151   time_elapsed = 29450.328   time_remaining = 105643\n",
      "Epoch 110 Batch  139/139   train_loss = 4.125   time_elapsed = 29719.240   time_remaining = 105368\n",
      "Epoch 111 Batch  139/139   train_loss = 4.103   time_elapsed = 29988.429   time_remaining = 105095\n",
      "Model Trained and Saved\n",
      "Epoch 112 Batch  139/139   train_loss = 4.100   time_elapsed = 30263.121   time_remaining = 104840\n",
      "Epoch 113 Batch  139/139   train_loss = 4.090   time_elapsed = 30531.726   time_remaining = 104564\n",
      "Epoch 114 Batch  139/139   train_loss = 4.072   time_elapsed = 30799.311   time_remaining = 104285\n",
      "Epoch 115 Batch  139/139   train_loss = 4.061   time_elapsed = 31066.319   time_remaining = 104005\n",
      "Epoch 116 Batch  139/139   train_loss = 4.060   time_elapsed = 31334.672   time_remaining = 103729\n",
      "Epoch 117 Batch  139/139   train_loss = 4.078   time_elapsed = 31605.110   time_remaining = 103459\n",
      "Epoch 118 Batch  139/139   train_loss = 4.092   time_elapsed = 31872.931   time_remaining = 103182\n",
      "Epoch 119 Batch  139/139   train_loss = 4.058   time_elapsed = 32141.771   time_remaining = 102908\n",
      "Epoch 120 Batch  139/139   train_loss = 4.023   time_elapsed = 32412.147   time_remaining = 102638\n",
      "Epoch 121 Batch  139/139   train_loss = 4.038   time_elapsed = 32680.453   time_remaining = 102363\n",
      "Model Trained and Saved\n",
      "Epoch 122 Batch  139/139   train_loss = 4.033   time_elapsed = 32956.671   time_remaining = 102112\n",
      "Epoch 123 Batch  139/139   train_loss = 4.020   time_elapsed = 33225.864   time_remaining = 101839\n",
      "Epoch 124 Batch  139/139   train_loss = 3.989   time_elapsed = 33486.935   time_remaining = 101541\n",
      "Epoch 125 Batch  139/139   train_loss = 3.967   time_elapsed = 33751.986   time_remaining = 101256\n",
      "Epoch 126 Batch  139/139   train_loss = 3.976   time_elapsed = 34021.200   time_remaining = 100984\n",
      "Epoch 127 Batch  139/139   train_loss = 3.990   time_elapsed = 34289.077   time_remaining = 100707\n",
      "Epoch 128 Batch  139/139   train_loss = 3.987   time_elapsed = 34558.366   time_remaining = 100435\n",
      "Epoch 129 Batch  139/139   train_loss = 3.970   time_elapsed = 34827.836   time_remaining = 100164\n",
      "Epoch 130 Batch  139/139   train_loss = 3.967   time_elapsed = 35096.746   time_remaining = 99891\n",
      "Epoch 131 Batch  139/139   train_loss = 3.961   time_elapsed = 35366.750   time_remaining = 99621\n",
      "Model Trained and Saved\n",
      "Epoch 132 Batch  139/139   train_loss = 3.943   time_elapsed = 35641.279   time_remaining = 99364\n",
      "Epoch 133 Batch  139/139   train_loss = 3.926   time_elapsed = 35909.859   time_remaining = 99090\n",
      "Epoch 134 Batch  139/139   train_loss = 3.916   time_elapsed = 36178.350   time_remaining = 98815\n",
      "Epoch 135 Batch  139/139   train_loss = 3.922   time_elapsed = 36446.650   time_remaining = 98541\n",
      "Epoch 136 Batch  139/139   train_loss = 3.926   time_elapsed = 36715.905   time_remaining = 98269\n",
      "Epoch 137 Batch  139/139   train_loss = 3.924   time_elapsed = 36986.107   time_remaining = 98000\n",
      "Epoch 138 Batch  139/139   train_loss = 3.917   time_elapsed = 37256.261   time_remaining = 97730\n",
      "Epoch 139 Batch  139/139   train_loss = 3.951   time_elapsed = 37526.933   time_remaining = 97462\n",
      "Epoch 140 Batch  139/139   train_loss = 3.929   time_elapsed = 37795.558   time_remaining = 97189\n",
      "Epoch 141 Batch  139/139   train_loss = 3.893   time_elapsed = 38064.674   time_remaining = 96916\n",
      "Model Trained and Saved\n",
      "Epoch 142 Batch  139/139   train_loss = 3.877   time_elapsed = 38340.268   time_remaining = 96661\n",
      "Epoch 143 Batch  139/139   train_loss = 3.876   time_elapsed = 38608.879   time_remaining = 96387\n",
      "Epoch 144 Batch  139/139   train_loss = 3.873   time_elapsed = 38877.224   time_remaining = 96113\n",
      "Epoch 145 Batch  139/139   train_loss = 3.883   time_elapsed = 39147.800   time_remaining = 95845\n",
      "Epoch 146 Batch  139/139   train_loss = 3.890   time_elapsed = 39417.609   time_remaining = 95574\n",
      "Epoch 147 Batch  139/139   train_loss = 3.861   time_elapsed = 39686.689   time_remaining = 95302\n",
      "Epoch 148 Batch  139/139   train_loss = 3.882   time_elapsed = 39955.377   time_remaining = 95029\n",
      "Epoch 149 Batch  139/139   train_loss = 3.860   time_elapsed = 40221.477   time_remaining = 94750\n",
      "Epoch 150 Batch  139/139   train_loss = 3.843   time_elapsed = 40482.147   time_remaining = 94458\n",
      "Epoch 151 Batch  139/139   train_loss = 3.852   time_elapsed = 40748.327   time_remaining = 94180\n",
      "Model Trained and Saved\n",
      "Epoch 152 Batch  139/139   train_loss = 3.847   time_elapsed = 41022.889   time_remaining = 93921\n",
      "Epoch 153 Batch  139/139   train_loss = 3.833   time_elapsed = 41291.186   time_remaining = 93647\n",
      "Epoch 154 Batch  139/139   train_loss = 3.835   time_elapsed = 41559.651   time_remaining = 93374\n",
      "Epoch 155 Batch  139/139   train_loss = 3.836   time_elapsed = 41828.694   time_remaining = 93103\n",
      "Epoch 156 Batch  139/139   train_loss = 3.817   time_elapsed = 42097.319   time_remaining = 92830\n",
      "Epoch 157 Batch  139/139   train_loss = 3.821   time_elapsed = 42365.364   time_remaining = 92556\n",
      "Epoch 158 Batch  139/139   train_loss = 3.824   time_elapsed = 42633.694   time_remaining = 92283\n",
      "Epoch 159 Batch  139/139   train_loss = 3.819   time_elapsed = 42903.123   time_remaining = 92012\n",
      "Epoch 160 Batch  139/139   train_loss = 3.824   time_elapsed = 43171.392   time_remaining = 91739\n",
      "Epoch 161 Batch  139/139   train_loss = 3.801   time_elapsed = 43440.042   time_remaining = 91467\n",
      "Model Trained and Saved\n",
      "Epoch 162 Batch  139/139   train_loss = 3.783   time_elapsed = 43714.230   time_remaining = 91206\n",
      "Epoch 163 Batch  139/139   train_loss = 3.797   time_elapsed = 43981.880   time_remaining = 90932\n",
      "Epoch 164 Batch  139/139   train_loss = 3.795   time_elapsed = 44251.293   time_remaining = 90661\n",
      "Epoch 165 Batch  139/139   train_loss = 3.782   time_elapsed = 44520.637   time_remaining = 90390\n",
      "Epoch 166 Batch  139/139   train_loss = 3.782   time_elapsed = 44789.247   time_remaining = 90118\n",
      "Epoch 167 Batch  139/139   train_loss = 3.767   time_elapsed = 45059.127   time_remaining = 89848\n",
      "Epoch 168 Batch  139/139   train_loss = 3.760   time_elapsed = 45327.298   time_remaining = 89575\n",
      "Epoch 169 Batch  139/139   train_loss = 3.769   time_elapsed = 45596.361   time_remaining = 89304\n",
      "Epoch 170 Batch  139/139   train_loss = 3.779   time_elapsed = 45865.519   time_remaining = 89033\n",
      "Epoch 171 Batch  139/139   train_loss = 3.810   time_elapsed = 46134.800   time_remaining = 88762\n",
      "Model Trained and Saved\n",
      "Epoch 172 Batch  139/139   train_loss = 3.830   time_elapsed = 46410.262   time_remaining = 88503\n",
      "Epoch 173 Batch  139/139   train_loss = 3.777   time_elapsed = 46679.089   time_remaining = 88232\n",
      "Epoch 174 Batch  139/139   train_loss = 3.743   time_elapsed = 46949.237   time_remaining = 87962\n",
      "Epoch 175 Batch  139/139   train_loss = 3.744   time_elapsed = 47213.998   time_remaining = 87683\n",
      "Epoch 176 Batch  139/139   train_loss = 3.726   time_elapsed = 47474.969   time_remaining = 87397\n",
      "Epoch 177 Batch  139/139   train_loss = 3.738   time_elapsed = 47743.401   time_remaining = 87125\n",
      "Epoch 178 Batch  139/139   train_loss = 3.741   time_elapsed = 48011.083   time_remaining = 86852\n",
      "Epoch 179 Batch  139/139   train_loss = 3.727   time_elapsed = 48280.131   time_remaining = 86581\n",
      "Epoch 180 Batch  139/139   train_loss = 3.704   time_elapsed = 48548.402   time_remaining = 86308\n",
      "Epoch 181 Batch  139/139   train_loss = 3.731   time_elapsed = 48818.009   time_remaining = 86038\n",
      "Model Trained and Saved\n",
      "Epoch 182 Batch  139/139   train_loss = 3.745   time_elapsed = 49093.181   time_remaining = 85778\n",
      "Epoch 183 Batch  139/139   train_loss = 3.728   time_elapsed = 49361.948   time_remaining = 85507\n",
      "Epoch 184 Batch  139/139   train_loss = 3.705   time_elapsed = 49631.469   time_remaining = 85237\n",
      "Epoch 185 Batch  139/139   train_loss = 3.704   time_elapsed = 49901.008   time_remaining = 84967\n",
      "Epoch 186 Batch  139/139   train_loss = 3.713   time_elapsed = 50170.689   time_remaining = 84697\n",
      "Epoch 187 Batch  139/139   train_loss = 3.711   time_elapsed = 50439.801   time_remaining = 84426\n",
      "Epoch 188 Batch  139/139   train_loss = 3.707   time_elapsed = 50709.598   time_remaining = 84156\n",
      "Epoch 189 Batch  139/139   train_loss = 3.688   time_elapsed = 50978.843   time_remaining = 83886\n",
      "Epoch 190 Batch  139/139   train_loss = 3.680   time_elapsed = 51248.153   time_remaining = 83615\n",
      "Epoch 191 Batch  139/139   train_loss = 3.691   time_elapsed = 51517.523   time_remaining = 83345\n",
      "Model Trained and Saved\n",
      "Epoch 192 Batch  139/139   train_loss = 3.673   time_elapsed = 51792.790   time_remaining = 83084\n",
      "Epoch 193 Batch  139/139   train_loss = 3.677   time_elapsed = 52062.130   time_remaining = 82814\n",
      "Epoch 194 Batch  139/139   train_loss = 3.660   time_elapsed = 52331.555   time_remaining = 82544\n",
      "Epoch 195 Batch  139/139   train_loss = 3.659   time_elapsed = 52600.028   time_remaining = 82272\n",
      "Epoch 196 Batch  139/139   train_loss = 3.659   time_elapsed = 52869.206   time_remaining = 82001\n",
      "Epoch 197 Batch  139/139   train_loss = 3.655   time_elapsed = 53138.997   time_remaining = 81732\n",
      "Epoch 198 Batch  139/139   train_loss = 3.678   time_elapsed = 53406.626   time_remaining = 81459\n",
      "Epoch 199 Batch  139/139   train_loss = 3.684   time_elapsed = 53675.147   time_remaining = 81187\n",
      "Epoch 200 Batch  139/139   train_loss = 3.695   time_elapsed = 53945.394   time_remaining = 80918\n",
      "Epoch 201 Batch  139/139   train_loss = 3.696   time_elapsed = 54209.396   time_remaining = 80640\n",
      "Model Trained and Saved\n",
      "Epoch 202 Batch  139/139   train_loss = 3.681   time_elapsed = 54477.669   time_remaining = 80368\n",
      "Epoch 203 Batch  139/139   train_loss = 3.673   time_elapsed = 54746.204   time_remaining = 80097\n",
      "Epoch 204 Batch  139/139   train_loss = 3.652   time_elapsed = 55015.071   time_remaining = 79826\n",
      "Epoch 205 Batch  139/139   train_loss = 3.630   time_elapsed = 55284.770   time_remaining = 79556\n",
      "Epoch 206 Batch  139/139   train_loss = 3.633   time_elapsed = 55553.133   time_remaining = 79285\n",
      "Epoch 207 Batch  139/139   train_loss = 3.641   time_elapsed = 55822.761   time_remaining = 79015\n",
      "Epoch 208 Batch  139/139   train_loss = 3.628   time_elapsed = 56092.496   time_remaining = 78745\n",
      "Epoch 209 Batch  139/139   train_loss = 3.611   time_elapsed = 56361.756   time_remaining = 78475\n",
      "Epoch 210 Batch  139/139   train_loss = 3.622   time_elapsed = 56630.059   time_remaining = 78203\n",
      "Epoch 211 Batch  139/139   train_loss = 3.595   time_elapsed = 56899.434   time_remaining = 77933\n",
      "Model Trained and Saved\n",
      "Epoch 212 Batch  139/139   train_loss = 3.606   time_elapsed = 57174.920   time_remaining = 77672\n",
      "Epoch 213 Batch  139/139   train_loss = 3.620   time_elapsed = 57444.485   time_remaining = 77402\n",
      "Epoch 214 Batch  139/139   train_loss = 3.620   time_elapsed = 57714.261   time_remaining = 77132\n",
      "Epoch 215 Batch  139/139   train_loss = 3.616   time_elapsed = 57982.735   time_remaining = 76861\n",
      "Epoch 216 Batch  139/139   train_loss = 3.606   time_elapsed = 58251.354   time_remaining = 76590\n",
      "Epoch 217 Batch  139/139   train_loss = 3.598   time_elapsed = 58522.072   time_remaining = 76321\n",
      "Epoch 218 Batch  139/139   train_loss = 3.615   time_elapsed = 58792.896   time_remaining = 76053\n",
      "Epoch 219 Batch  139/139   train_loss = 3.624   time_elapsed = 59062.816   time_remaining = 75784\n",
      "Epoch 220 Batch  139/139   train_loss = 3.616   time_elapsed = 59332.928   time_remaining = 75515\n",
      "Epoch 221 Batch  139/139   train_loss = 3.608   time_elapsed = 59602.606   time_remaining = 75245\n",
      "Model Trained and Saved\n",
      "Epoch 222 Batch  139/139   train_loss = 3.586   time_elapsed = 59879.193   time_remaining = 74984\n",
      "Epoch 223 Batch  139/139   train_loss = 3.568   time_elapsed = 60148.314   time_remaining = 74713\n",
      "Epoch 224 Batch  139/139   train_loss = 3.577   time_elapsed = 60417.842   time_remaining = 74443\n",
      "Epoch 225 Batch  139/139   train_loss = 3.588   time_elapsed = 60687.972   time_remaining = 74174\n",
      "Epoch 226 Batch  139/139   train_loss = 3.574   time_elapsed = 60957.209   time_remaining = 73904\n",
      "Epoch 227 Batch  139/139   train_loss = 3.583   time_elapsed = 61219.368   time_remaining = 73625\n",
      "Epoch 228 Batch  139/139   train_loss = 3.592   time_elapsed = 61482.958   time_remaining = 73348\n",
      "Epoch 229 Batch  139/139   train_loss = 3.579   time_elapsed = 61751.722   time_remaining = 73077\n",
      "Epoch 230 Batch  139/139   train_loss = 3.588   time_elapsed = 62020.276   time_remaining = 72806\n",
      "Epoch 231 Batch  139/139   train_loss = 3.582   time_elapsed = 62289.885   time_remaining = 72537\n",
      "Model Trained and Saved\n",
      "Epoch 232 Batch  139/139   train_loss = 3.569   time_elapsed = 62565.016   time_remaining = 72273\n",
      "Epoch 233 Batch  139/139   train_loss = 3.566   time_elapsed = 62833.628   time_remaining = 72002\n",
      "Epoch 234 Batch  139/139   train_loss = 3.567   time_elapsed = 63103.320   time_remaining = 71733\n",
      "Epoch 235 Batch  139/139   train_loss = 3.562   time_elapsed = 63372.409   time_remaining = 71463\n",
      "Epoch 236 Batch  139/139   train_loss = 3.550   time_elapsed = 63642.222   time_remaining = 71193\n",
      "Epoch 237 Batch  139/139   train_loss = 3.551   time_elapsed = 63910.547   time_remaining = 70922\n",
      "Epoch 238 Batch  139/139   train_loss = 3.563   time_elapsed = 64178.627   time_remaining = 70650\n",
      "Epoch 239 Batch  139/139   train_loss = 3.552   time_elapsed = 64447.589   time_remaining = 70380\n",
      "Epoch 240 Batch  139/139   train_loss = 3.540   time_elapsed = 64716.321   time_remaining = 70109\n",
      "Epoch 241 Batch  139/139   train_loss = 3.545   time_elapsed = 64986.167   time_remaining = 69840\n",
      "Model Trained and Saved\n",
      "Epoch 242 Batch  139/139   train_loss = 3.564   time_elapsed = 65261.294   time_remaining = 69576\n",
      "Epoch 243 Batch  139/139   train_loss = 3.574   time_elapsed = 65529.390   time_remaining = 69305\n",
      "Epoch 244 Batch  139/139   train_loss = 3.567   time_elapsed = 65797.688   time_remaining = 69034\n",
      "Epoch 245 Batch  139/139   train_loss = 3.564   time_elapsed = 66065.960   time_remaining = 68763\n",
      "Epoch 246 Batch  139/139   train_loss = 3.544   time_elapsed = 66334.901   time_remaining = 68492\n",
      "Epoch 247 Batch  139/139   train_loss = 3.541   time_elapsed = 66605.389   time_remaining = 68223\n",
      "Epoch 248 Batch  139/139   train_loss = 3.543   time_elapsed = 66875.486   time_remaining = 67954\n",
      "Epoch 249 Batch  139/139   train_loss = 3.529   time_elapsed = 67144.581   time_remaining = 67684\n",
      "Epoch 250 Batch  139/139   train_loss = 3.525   time_elapsed = 67412.450   time_remaining = 67412\n",
      "Epoch 251 Batch  139/139   train_loss = 3.525   time_elapsed = 67679.745   time_remaining = 67140\n",
      "Model Trained and Saved\n",
      "Epoch 252 Batch  139/139   train_loss = 3.536   time_elapsed = 67955.338   time_remaining = 66877\n",
      "Epoch 253 Batch  139/139   train_loss = 3.528   time_elapsed = 68216.290   time_remaining = 66599\n",
      "Epoch 254 Batch  139/139   train_loss = 3.517   time_elapsed = 68476.660   time_remaining = 66320\n",
      "Epoch 255 Batch  139/139   train_loss = 3.516   time_elapsed = 68736.158   time_remaining = 66041\n",
      "Epoch 256 Batch  139/139   train_loss = 3.522   time_elapsed = 68995.566   time_remaining = 65761\n",
      "Epoch 257 Batch  139/139   train_loss = 3.498   time_elapsed = 69254.961   time_remaining = 65482\n",
      "Epoch 258 Batch  139/139   train_loss = 3.500   time_elapsed = 69514.356   time_remaining = 65203\n",
      "Epoch 259 Batch  139/139   train_loss = 3.523   time_elapsed = 69773.719   time_remaining = 64925\n",
      "Epoch 260 Batch  139/139   train_loss = 3.556   time_elapsed = 70033.058   time_remaining = 64646\n",
      "Epoch 261 Batch  139/139   train_loss = 3.535   time_elapsed = 70292.491   time_remaining = 64367\n",
      "Model Trained and Saved\n",
      "Epoch 262 Batch  139/139   train_loss = 3.508   time_elapsed = 70559.315   time_remaining = 64096\n",
      "Epoch 263 Batch  139/139   train_loss = 3.512   time_elapsed = 70819.138   time_remaining = 63818\n",
      "Epoch 264 Batch  139/139   train_loss = 3.481   time_elapsed = 71078.100   time_remaining = 63540\n",
      "Epoch 265 Batch  139/139   train_loss = 3.497   time_elapsed = 71337.151   time_remaining = 63261\n",
      "Epoch 266 Batch  139/139   train_loss = 3.480   time_elapsed = 71596.103   time_remaining = 62983\n",
      "Epoch 267 Batch  139/139   train_loss = 3.467   time_elapsed = 71855.123   time_remaining = 62705\n",
      "Epoch 268 Batch  139/139   train_loss = 3.465   time_elapsed = 72114.178   time_remaining = 62427\n",
      "Epoch 269 Batch  139/139   train_loss = 3.478   time_elapsed = 72373.274   time_remaining = 62150\n",
      "Epoch 270 Batch  139/139   train_loss = 3.467   time_elapsed = 72632.417   time_remaining = 61872\n",
      "Epoch 271 Batch  139/139   train_loss = 3.483   time_elapsed = 72891.404   time_remaining = 61595\n",
      "Model Trained and Saved\n",
      "Epoch 272 Batch  139/139   train_loss = 3.471   time_elapsed = 73158.128   time_remaining = 61324\n",
      "Epoch 273 Batch  139/139   train_loss = 3.477   time_elapsed = 73418.797   time_remaining = 61048\n",
      "Epoch 274 Batch  139/139   train_loss = 3.477   time_elapsed = 73679.371   time_remaining = 60772\n",
      "Epoch 275 Batch  139/139   train_loss = 3.484   time_elapsed = 73939.968   time_remaining = 60496\n",
      "Epoch 276 Batch  139/139   train_loss = 3.482   time_elapsed = 74200.616   time_remaining = 60221\n",
      "Epoch 277 Batch  139/139   train_loss = 3.466   time_elapsed = 74461.238   time_remaining = 59945\n",
      "Epoch 278 Batch  139/139   train_loss = 3.467   time_elapsed = 74721.905   time_remaining = 59670\n",
      "Epoch 279 Batch  139/139   train_loss = 3.462   time_elapsed = 74982.569   time_remaining = 59395\n",
      "Epoch 280 Batch  139/139   train_loss = 3.451   time_elapsed = 75243.185   time_remaining = 59120\n",
      "Epoch 281 Batch  139/139   train_loss = 3.442   time_elapsed = 75503.777   time_remaining = 58845\n",
      "Model Trained and Saved\n",
      "Epoch 282 Batch  139/139   train_loss = 3.451   time_elapsed = 75770.845   time_remaining = 58575\n",
      "Epoch 283 Batch  139/139   train_loss = 3.454   time_elapsed = 76031.526   time_remaining = 58300\n",
      "Epoch 284 Batch  139/139   train_loss = 3.457   time_elapsed = 76292.155   time_remaining = 58025\n",
      "Epoch 285 Batch  139/139   train_loss = 3.463   time_elapsed = 76552.764   time_remaining = 57750\n",
      "Epoch 286 Batch  139/139   train_loss = 3.443   time_elapsed = 76813.400   time_remaining = 57476\n",
      "Epoch 287 Batch  139/139   train_loss = 3.437   time_elapsed = 77074.103   time_remaining = 57201\n",
      "Epoch 288 Batch  139/139   train_loss = 3.445   time_elapsed = 77334.825   time_remaining = 56927\n",
      "Epoch 289 Batch  139/139   train_loss = 3.450   time_elapsed = 77595.506   time_remaining = 56653\n",
      "Epoch 290 Batch  139/139   train_loss = 3.435   time_elapsed = 77856.093   time_remaining = 56379\n",
      "Epoch 291 Batch  139/139   train_loss = 3.439   time_elapsed = 78116.750   time_remaining = 56104\n",
      "Model Trained and Saved\n",
      "Epoch 292 Batch  139/139   train_loss = 3.452   time_elapsed = 78383.845   time_remaining = 55835\n",
      "Epoch 293 Batch  139/139   train_loss = 3.460   time_elapsed = 78644.564   time_remaining = 55561\n",
      "Epoch 294 Batch  139/139   train_loss = 3.459   time_elapsed = 78905.235   time_remaining = 55287\n",
      "Epoch 295 Batch  139/139   train_loss = 3.458   time_elapsed = 79165.925   time_remaining = 55014\n",
      "Epoch 296 Batch  139/139   train_loss = 3.481   time_elapsed = 79426.575   time_remaining = 54740\n",
      "Epoch 297 Batch  139/139   train_loss = 3.485   time_elapsed = 79687.231   time_remaining = 54466\n",
      "Epoch 298 Batch  139/139   train_loss = 3.478   time_elapsed = 79947.891   time_remaining = 54193\n",
      "Epoch 299 Batch  139/139   train_loss = 3.457   time_elapsed = 80208.589   time_remaining = 53919\n",
      "Epoch 300 Batch  139/139   train_loss = 3.455   time_elapsed = 80469.279   time_remaining = 53646\n",
      "Epoch 301 Batch  139/139   train_loss = 3.425   time_elapsed = 80730.066   time_remaining = 53373\n",
      "Model Trained and Saved\n",
      "Epoch 302 Batch  139/139   train_loss = 3.425   time_elapsed = 80997.139   time_remaining = 53104\n",
      "Epoch 303 Batch  139/139   train_loss = 3.423   time_elapsed = 81257.747   time_remaining = 52831\n",
      "Epoch 304 Batch  139/139   train_loss = 3.431   time_elapsed = 81518.512   time_remaining = 52558\n",
      "Epoch 305 Batch  139/139   train_loss = 3.457   time_elapsed = 81779.242   time_remaining = 52285\n",
      "Epoch 306 Batch  139/139   train_loss = 3.442   time_elapsed = 82039.986   time_remaining = 52012\n",
      "Epoch 307 Batch  139/139   train_loss = 3.427   time_elapsed = 82300.774   time_remaining = 51740\n",
      "Epoch 308 Batch  139/139   train_loss = 3.417   time_elapsed = 82561.573   time_remaining = 51467\n",
      "Epoch 309 Batch  139/139   train_loss = 3.395   time_elapsed = 82822.326   time_remaining = 51194\n",
      "Epoch 310 Batch  139/139   train_loss = 3.395   time_elapsed = 83083.021   time_remaining = 50922\n",
      "Epoch 311 Batch  139/139   train_loss = 3.404   time_elapsed = 83343.711   time_remaining = 50649\n",
      "Model Trained and Saved\n",
      "Epoch 312 Batch  139/139   train_loss = 3.392   time_elapsed = 83610.839   time_remaining = 50381\n",
      "Epoch 313 Batch  139/139   train_loss = 3.400   time_elapsed = 83871.541   time_remaining = 50109\n",
      "Epoch 314 Batch  139/139   train_loss = 3.395   time_elapsed = 84132.224   time_remaining = 49836\n",
      "Epoch 315 Batch  139/139   train_loss = 3.394   time_elapsed = 84392.827   time_remaining = 49564\n",
      "Epoch 316 Batch  139/139   train_loss = 3.404   time_elapsed = 84653.389   time_remaining = 49292\n",
      "Epoch 317 Batch  139/139   train_loss = 3.395   time_elapsed = 84913.942   time_remaining = 49020\n",
      "Epoch 318 Batch  139/139   train_loss = 3.389   time_elapsed = 85174.601   time_remaining = 48748\n",
      "Epoch 319 Batch  139/139   train_loss = 3.396   time_elapsed = 85435.286   time_remaining = 48476\n",
      "Epoch 320 Batch  139/139   train_loss = 3.374   time_elapsed = 85695.935   time_remaining = 48204\n",
      "Epoch 321 Batch  139/139   train_loss = 3.383   time_elapsed = 85956.658   time_remaining = 47932\n",
      "Model Trained and Saved\n",
      "Epoch 322 Batch  139/139   train_loss = 3.395   time_elapsed = 86222.811   time_remaining = 47664\n",
      "Epoch 323 Batch  139/139   train_loss = 3.400   time_elapsed = 86482.656   time_remaining = 47391\n",
      "Epoch 324 Batch  139/139   train_loss = 3.397   time_elapsed = 86742.565   time_remaining = 47119\n",
      "Epoch 325 Batch  139/139   train_loss = 3.398   time_elapsed = 87002.465   time_remaining = 46847\n",
      "Epoch 326 Batch  139/139   train_loss = 3.409   time_elapsed = 87262.312   time_remaining = 46576\n",
      "Epoch 327 Batch  139/139   train_loss = 3.400   time_elapsed = 87522.182   time_remaining = 46304\n",
      "Epoch 328 Batch  139/139   train_loss = 3.380   time_elapsed = 87782.186   time_remaining = 46032\n",
      "Epoch 329 Batch  139/139   train_loss = 3.376   time_elapsed = 88041.983   time_remaining = 45760\n",
      "Epoch 330 Batch  139/139   train_loss = 3.364   time_elapsed = 88301.786   time_remaining = 45489\n",
      "Epoch 331 Batch  139/139   train_loss = 3.356   time_elapsed = 88561.594   time_remaining = 45217\n",
      "Model Trained and Saved\n",
      "Epoch 332 Batch  139/139   train_loss = 3.362   time_elapsed = 88828.442   time_remaining = 44949\n",
      "Epoch 333 Batch  139/139   train_loss = 3.363   time_elapsed = 89089.169   time_remaining = 44678\n",
      "Epoch 334 Batch  139/139   train_loss = 3.354   time_elapsed = 89349.759   time_remaining = 44407\n",
      "Epoch 335 Batch  139/139   train_loss = 3.370   time_elapsed = 89610.509   time_remaining = 44137\n",
      "Epoch 336 Batch  139/139   train_loss = 3.353   time_elapsed = 89871.137   time_remaining = 43866\n",
      "Epoch 337 Batch  139/139   train_loss = 3.351   time_elapsed = 90131.740   time_remaining = 43595\n",
      "Epoch 338 Batch  139/139   train_loss = 3.350   time_elapsed = 90392.283   time_remaining = 43324\n",
      "Epoch 339 Batch  139/139   train_loss = 3.351   time_elapsed = 90652.831   time_remaining = 43053\n",
      "Epoch 340 Batch  139/139   train_loss = 3.363   time_elapsed = 90913.486   time_remaining = 42783\n",
      "Epoch 341 Batch  139/139   train_loss = 3.348   time_elapsed = 91173.904   time_remaining = 42512\n",
      "Model Trained and Saved\n",
      "Epoch 342 Batch  139/139   train_loss = 3.349   time_elapsed = 91440.303   time_remaining = 42244\n",
      "Epoch 343 Batch  139/139   train_loss = 3.359   time_elapsed = 91700.301   time_remaining = 41974\n",
      "Epoch 344 Batch  139/139   train_loss = 3.360   time_elapsed = 91960.328   time_remaining = 41703\n",
      "Epoch 345 Batch  139/139   train_loss = 3.360   time_elapsed = 92220.423   time_remaining = 41432\n",
      "Epoch 346 Batch  139/139   train_loss = 3.373   time_elapsed = 92480.552   time_remaining = 41162\n",
      "Epoch 347 Batch  139/139   train_loss = 3.391   time_elapsed = 92740.692   time_remaining = 40891\n",
      "Epoch 348 Batch  139/139   train_loss = 3.404   time_elapsed = 93000.637   time_remaining = 40621\n",
      "Epoch 349 Batch  139/139   train_loss = 3.371   time_elapsed = 93260.699   time_remaining = 40351\n",
      "Epoch 350 Batch  139/139   train_loss = 3.341   time_elapsed = 93520.704   time_remaining = 40080\n",
      "Epoch 351 Batch  139/139   train_loss = 3.364   time_elapsed = 93780.758   time_remaining = 39810\n",
      "Model Trained and Saved\n",
      "Epoch 352 Batch  139/139   train_loss = 3.327   time_elapsed = 94047.148   time_remaining = 39543\n",
      "Epoch 353 Batch  139/139   train_loss = 3.323   time_elapsed = 94307.180   time_remaining = 39272\n",
      "Epoch 354 Batch  139/139   train_loss = 3.333   time_elapsed = 94567.194   time_remaining = 39002\n",
      "Epoch 355 Batch  139/139   train_loss = 3.323   time_elapsed = 94827.287   time_remaining = 38732\n",
      "Epoch 356 Batch  139/139   train_loss = 3.325   time_elapsed = 95087.339   time_remaining = 38462\n",
      "Epoch 357 Batch  139/139   train_loss = 3.336   time_elapsed = 95347.419   time_remaining = 38192\n",
      "Epoch 358 Batch  139/139   train_loss = 3.327   time_elapsed = 95607.364   time_remaining = 37922\n",
      "Epoch 359 Batch  139/139   train_loss = 3.309   time_elapsed = 95867.505   time_remaining = 37653\n",
      "Epoch 360 Batch  139/139   train_loss = 3.311   time_elapsed = 96127.509   time_remaining = 37383\n",
      "Epoch 361 Batch  139/139   train_loss = 3.323   time_elapsed = 96387.510   time_remaining = 37113\n",
      "Model Trained and Saved\n",
      "Epoch 362 Batch  139/139   train_loss = 3.346   time_elapsed = 96654.461   time_remaining = 36846\n",
      "Epoch 363 Batch  139/139   train_loss = 3.349   time_elapsed = 96915.082   time_remaining = 36577\n",
      "Epoch 364 Batch  139/139   train_loss = 3.339   time_elapsed = 97175.600   time_remaining = 36307\n",
      "Epoch 365 Batch  139/139   train_loss = 3.324   time_elapsed = 97436.172   time_remaining = 36038\n",
      "Epoch 366 Batch  139/139   train_loss = 3.341   time_elapsed = 97695.801   time_remaining = 35768\n",
      "Epoch 367 Batch  139/139   train_loss = 3.339   time_elapsed = 97954.010   time_remaining = 35498\n",
      "Epoch 368 Batch  139/139   train_loss = 3.324   time_elapsed = 98213.134   time_remaining = 35229\n",
      "Epoch 369 Batch  139/139   train_loss = 3.342   time_elapsed = 98472.243   time_remaining = 34959\n",
      "Epoch 370 Batch  139/139   train_loss = 3.328   time_elapsed = 98731.346   time_remaining = 34689\n",
      "Epoch 371 Batch  139/139   train_loss = 3.350   time_elapsed = 98990.491   time_remaining = 34420\n",
      "Model Trained and Saved\n",
      "Epoch 372 Batch  139/139   train_loss = 3.337   time_elapsed = 99256.594   time_remaining = 34153\n",
      "Epoch 373 Batch  139/139   train_loss = 3.305   time_elapsed = 99516.700   time_remaining = 33884\n",
      "Epoch 374 Batch  139/139   train_loss = 3.302   time_elapsed = 99776.688   time_remaining = 33615\n",
      "Epoch 375 Batch  139/139   train_loss = 3.313   time_elapsed = 100036.673   time_remaining = 33346\n",
      "Epoch 376 Batch  139/139   train_loss = 3.325   time_elapsed = 100296.591   time_remaining = 33077\n",
      "Epoch 377 Batch  139/139   train_loss = 3.317   time_elapsed = 100556.611   time_remaining = 32808\n",
      "Epoch 378 Batch  139/139   train_loss = 3.305   time_elapsed = 100816.596   time_remaining = 32539\n",
      "Epoch 379 Batch  139/139   train_loss = 3.310   time_elapsed = 101076.603   time_remaining = 32270\n",
      "Epoch 380 Batch  139/139   train_loss = 3.295   time_elapsed = 101336.668   time_remaining = 32001\n",
      "Epoch 381 Batch  139/139   train_loss = 3.293   time_elapsed = 101596.812   time_remaining = 31732\n",
      "Model Trained and Saved\n",
      "Epoch 382 Batch  139/139   train_loss = 3.297   time_elapsed = 101863.192   time_remaining = 31466\n",
      "Epoch 383 Batch  139/139   train_loss = 3.293   time_elapsed = 102123.312   time_remaining = 31197\n",
      "Epoch 384 Batch  139/139   train_loss = 3.301   time_elapsed = 102383.405   time_remaining = 30928\n",
      "Epoch 385 Batch  139/139   train_loss = 3.304   time_elapsed = 102643.479   time_remaining = 30660\n",
      "Epoch 386 Batch  139/139   train_loss = 3.300   time_elapsed = 102903.480   time_remaining = 30391\n",
      "Epoch 387 Batch  139/139   train_loss = 3.309   time_elapsed = 103163.457   time_remaining = 30123\n",
      "Epoch 388 Batch  139/139   train_loss = 3.309   time_elapsed = 103423.470   time_remaining = 29854\n",
      "Epoch 389 Batch  139/139   train_loss = 3.293   time_elapsed = 103683.443   time_remaining = 29586\n",
      "Epoch 390 Batch  139/139   train_loss = 3.298   time_elapsed = 103943.265   time_remaining = 29317\n",
      "Epoch 391 Batch  139/139   train_loss = 3.309   time_elapsed = 104203.244   time_remaining = 29049\n",
      "Model Trained and Saved\n",
      "Epoch 392 Batch  139/139   train_loss = 3.293   time_elapsed = 104469.120   time_remaining = 28782\n",
      "Epoch 393 Batch  139/139   train_loss = 3.290   time_elapsed = 104728.730   time_remaining = 28514\n",
      "Epoch 394 Batch  139/139   train_loss = 3.291   time_elapsed = 104988.357   time_remaining = 28246\n",
      "Epoch 395 Batch  139/139   train_loss = 3.265   time_elapsed = 105248.027   time_remaining = 27977\n",
      "Epoch 396 Batch  139/139   train_loss = 3.277   time_elapsed = 105507.773   time_remaining = 27709\n",
      "Epoch 397 Batch  139/139   train_loss = 3.285   time_elapsed = 105767.493   time_remaining = 27441\n",
      "Epoch 398 Batch  139/139   train_loss = 3.273   time_elapsed = 106027.090   time_remaining = 27173\n",
      "Epoch 399 Batch  139/139   train_loss = 3.288   time_elapsed = 106286.795   time_remaining = 26905\n",
      "Epoch 400 Batch  139/139   train_loss = 3.291   time_elapsed = 106546.419   time_remaining = 26637\n",
      "Epoch 401 Batch  139/139   train_loss = 3.304   time_elapsed = 106806.106   time_remaining = 26369\n",
      "Model Trained and Saved\n",
      "Epoch 402 Batch  139/139   train_loss = 3.280   time_elapsed = 107072.695   time_remaining = 26102\n",
      "Epoch 403 Batch  139/139   train_loss = 3.312   time_elapsed = 107333.133   time_remaining = 25835\n",
      "Epoch 404 Batch  139/139   train_loss = 3.308   time_elapsed = 107593.588   time_remaining = 25567\n",
      "Epoch 405 Batch  139/139   train_loss = 3.296   time_elapsed = 107854.094   time_remaining = 25299\n",
      "Epoch 406 Batch  139/139   train_loss = 3.305   time_elapsed = 108114.615   time_remaining = 25031\n",
      "Epoch 407 Batch  139/139   train_loss = 3.306   time_elapsed = 108375.152   time_remaining = 24764\n",
      "Epoch 408 Batch  139/139   train_loss = 3.291   time_elapsed = 108635.682   time_remaining = 24496\n",
      "Epoch 409 Batch  139/139   train_loss = 3.281   time_elapsed = 108896.129   time_remaining = 24229\n",
      "Epoch 410 Batch  139/139   train_loss = 3.278   time_elapsed = 109156.483   time_remaining = 23961\n",
      "Epoch 411 Batch  139/139   train_loss = 3.274   time_elapsed = 109417.015   time_remaining = 23694\n",
      "Model Trained and Saved\n",
      "Epoch 412 Batch  139/139   train_loss = 3.282   time_elapsed = 109683.839   time_remaining = 23428\n",
      "Epoch 413 Batch  139/139   train_loss = 3.272   time_elapsed = 109944.279   time_remaining = 23160\n",
      "Epoch 414 Batch  139/139   train_loss = 3.287   time_elapsed = 110204.744   time_remaining = 22893\n",
      "Epoch 415 Batch  139/139   train_loss = 3.268   time_elapsed = 110465.284   time_remaining = 22625\n",
      "Epoch 416 Batch  139/139   train_loss = 3.251   time_elapsed = 110725.766   time_remaining = 22358\n",
      "Epoch 417 Batch  139/139   train_loss = 3.261   time_elapsed = 110986.254   time_remaining = 22091\n",
      "Epoch 418 Batch  139/139   train_loss = 3.272   time_elapsed = 111246.833   time_remaining = 21824\n",
      "Epoch 419 Batch  139/139   train_loss = 3.287   time_elapsed = 111507.443   time_remaining = 21556\n",
      "Epoch 420 Batch  139/139   train_loss = 3.277   time_elapsed = 111767.888   time_remaining = 21289\n",
      "Epoch 421 Batch  139/139   train_loss = 3.269   time_elapsed = 112028.443   time_remaining = 21022\n",
      "Model Trained and Saved\n",
      "Epoch 422 Batch  139/139   train_loss = 3.259   time_elapsed = 112294.381   time_remaining = 20756\n",
      "Epoch 423 Batch  139/139   train_loss = 3.265   time_elapsed = 112554.373   time_remaining = 20489\n",
      "Epoch 424 Batch  139/139   train_loss = 3.259   time_elapsed = 112814.510   time_remaining = 20221\n",
      "Epoch 425 Batch  139/139   train_loss = 3.263   time_elapsed = 113074.453   time_remaining = 19954\n",
      "Epoch 426 Batch  139/139   train_loss = 3.246   time_elapsed = 113334.399   time_remaining = 19687\n",
      "Epoch 427 Batch  139/139   train_loss = 3.264   time_elapsed = 113594.342   time_remaining = 19420\n",
      "Epoch 428 Batch  139/139   train_loss = 3.246   time_elapsed = 113854.296   time_remaining = 19153\n",
      "Epoch 429 Batch  139/139   train_loss = 3.240   time_elapsed = 114114.221   time_remaining = 18886\n",
      "Epoch 430 Batch  139/139   train_loss = 3.241   time_elapsed = 114374.119   time_remaining = 18619\n",
      "Epoch 431 Batch  139/139   train_loss = 3.241   time_elapsed = 114634.187   time_remaining = 18352\n",
      "Model Trained and Saved\n",
      "Epoch 432 Batch  139/139   train_loss = 3.246   time_elapsed = 114900.502   time_remaining = 18086\n",
      "Epoch 433 Batch  139/139   train_loss = 3.248   time_elapsed = 115160.679   time_remaining = 17819\n",
      "Epoch 434 Batch  139/139   train_loss = 3.253   time_elapsed = 115420.745   time_remaining = 17552\n",
      "Epoch 435 Batch  139/139   train_loss = 3.268   time_elapsed = 115680.709   time_remaining = 17286\n",
      "Epoch 436 Batch  139/139   train_loss = 3.251   time_elapsed = 115940.787   time_remaining = 17019\n",
      "Epoch 437 Batch  139/139   train_loss = 3.265   time_elapsed = 116200.835   time_remaining = 16752\n",
      "Epoch 438 Batch  139/139   train_loss = 3.269   time_elapsed = 116460.976   time_remaining = 16485\n",
      "Epoch 439 Batch  139/139   train_loss = 3.280   time_elapsed = 116721.147   time_remaining = 16219\n",
      "Epoch 440 Batch  139/139   train_loss = 3.278   time_elapsed = 116981.194   time_remaining = 15952\n",
      "Epoch 441 Batch  139/139   train_loss = 3.259   time_elapsed = 117241.216   time_remaining = 15685\n",
      "Model Trained and Saved\n",
      "Epoch 442 Batch  139/139   train_loss = 3.277   time_elapsed = 117507.563   time_remaining = 15420\n",
      "Epoch 443 Batch  139/139   train_loss = 3.270   time_elapsed = 117767.660   time_remaining = 15153\n",
      "Epoch 444 Batch  139/139   train_loss = 3.237   time_elapsed = 118027.638   time_remaining = 14886\n",
      "Epoch 445 Batch  139/139   train_loss = 3.226   time_elapsed = 118287.696   time_remaining = 14620\n",
      "Epoch 446 Batch  139/139   train_loss = 3.248   time_elapsed = 118547.647   time_remaining = 14353\n",
      "Epoch 447 Batch  139/139   train_loss = 3.214   time_elapsed = 118807.702   time_remaining = 14087\n",
      "Epoch 448 Batch  139/139   train_loss = 3.233   time_elapsed = 119067.672   time_remaining = 13820\n",
      "Epoch 449 Batch  139/139   train_loss = 3.233   time_elapsed = 119327.764   time_remaining = 13554\n",
      "Epoch 450 Batch  139/139   train_loss = 3.245   time_elapsed = 119587.913   time_remaining = 13288\n",
      "Epoch 451 Batch  139/139   train_loss = 3.229   time_elapsed = 119847.941   time_remaining = 13021\n",
      "Model Trained and Saved\n",
      "Epoch 452 Batch  139/139   train_loss = 3.235   time_elapsed = 120114.412   time_remaining = 12756\n",
      "Epoch 453 Batch  139/139   train_loss = 3.238   time_elapsed = 120374.545   time_remaining = 12489\n",
      "Epoch 454 Batch  139/139   train_loss = 3.245   time_elapsed = 120634.640   time_remaining = 12223\n",
      "Epoch 455 Batch  139/139   train_loss = 3.235   time_elapsed = 120894.742   time_remaining = 11957\n",
      "Epoch 456 Batch  139/139   train_loss = 3.227   time_elapsed = 121154.829   time_remaining = 11690\n",
      "Epoch 457 Batch  139/139   train_loss = 3.221   time_elapsed = 121415.002   time_remaining = 11424\n",
      "Epoch 458 Batch  139/139   train_loss = 3.230   time_elapsed = 121675.163   time_remaining = 11158\n",
      "Epoch 459 Batch  139/139   train_loss = 3.247   time_elapsed = 121935.244   time_remaining = 10892\n",
      "Epoch 460 Batch  139/139   train_loss = 3.227   time_elapsed = 122195.306   time_remaining = 10626\n",
      "Epoch 461 Batch  139/139   train_loss = 3.226   time_elapsed = 122455.459   time_remaining = 10360\n",
      "Model Trained and Saved\n",
      "Epoch 462 Batch  139/139   train_loss = 3.237   time_elapsed = 122721.929   time_remaining = 10094\n",
      "Epoch 463 Batch  139/139   train_loss = 3.222   time_elapsed = 122981.991   time_remaining = 9828\n",
      "Epoch 464 Batch  139/139   train_loss = 3.226   time_elapsed = 123242.004   time_remaining = 9562\n",
      "Epoch 465 Batch  139/139   train_loss = 3.226   time_elapsed = 123502.173   time_remaining = 9296\n",
      "Epoch 466 Batch  139/139   train_loss = 3.216   time_elapsed = 123762.076   time_remaining = 9030\n",
      "Epoch 467 Batch  139/139   train_loss = 3.210   time_elapsed = 124022.322   time_remaining = 8764\n",
      "Epoch 468 Batch  139/139   train_loss = 3.213   time_elapsed = 124282.356   time_remaining = 8498\n",
      "Epoch 469 Batch  139/139   train_loss = 3.219   time_elapsed = 124542.446   time_remaining = 8232\n",
      "Epoch 470 Batch  139/139   train_loss = 3.212   time_elapsed = 124802.456   time_remaining = 7966\n",
      "Epoch 471 Batch  139/139   train_loss = 3.217   time_elapsed = 125062.436   time_remaining = 7700\n",
      "Model Trained and Saved\n",
      "Epoch 472 Batch  139/139   train_loss = 3.216   time_elapsed = 125328.907   time_remaining = 7435\n",
      "Epoch 473 Batch  139/139   train_loss = 3.201   time_elapsed = 125589.168   time_remaining = 7169\n",
      "Epoch 474 Batch  139/139   train_loss = 3.215   time_elapsed = 125849.091   time_remaining = 6903\n",
      "Epoch 475 Batch  139/139   train_loss = 3.207   time_elapsed = 126109.136   time_remaining = 6637\n",
      "Epoch 476 Batch  139/139   train_loss = 3.210   time_elapsed = 126369.176   time_remaining = 6372\n",
      "Epoch 477 Batch  139/139   train_loss = 3.213   time_elapsed = 126629.159   time_remaining = 6106\n",
      "Epoch 478 Batch  139/139   train_loss = 3.211   time_elapsed = 126889.161   time_remaining = 5840\n",
      "Epoch 479 Batch  139/139   train_loss = 3.212   time_elapsed = 127149.225   time_remaining = 5574\n",
      "Epoch 480 Batch  139/139   train_loss = 3.217   time_elapsed = 127409.375   time_remaining = 5309\n",
      "Epoch 481 Batch  139/139   train_loss = 3.204   time_elapsed = 127669.485   time_remaining = 5043\n",
      "Model Trained and Saved\n",
      "Epoch 482 Batch  139/139   train_loss = 3.238   time_elapsed = 127936.402   time_remaining = 4778\n",
      "Epoch 483 Batch  139/139   train_loss = 3.223   time_elapsed = 128197.088   time_remaining = 4512\n",
      "Epoch 484 Batch  139/139   train_loss = 3.210   time_elapsed = 128457.836   time_remaining = 4247\n",
      "Epoch 485 Batch  139/139   train_loss = 3.224   time_elapsed = 128718.569   time_remaining = 3981\n",
      "Epoch 486 Batch  139/139   train_loss = 3.192   time_elapsed = 128979.248   time_remaining = 3715\n",
      "Epoch 487 Batch  139/139   train_loss = 3.205   time_elapsed = 129240.092   time_remaining = 3450\n",
      "Epoch 488 Batch  139/139   train_loss = 3.216   time_elapsed = 129500.711   time_remaining = 3184\n",
      "Epoch 489 Batch  139/139   train_loss = 3.210   time_elapsed = 129761.428   time_remaining = 2919\n",
      "Epoch 490 Batch  139/139   train_loss = 3.230   time_elapsed = 130022.069   time_remaining = 2654\n",
      "Epoch 491 Batch  139/139   train_loss = 3.217   time_elapsed = 130282.700   time_remaining = 2388\n",
      "Model Trained and Saved\n",
      "Epoch 492 Batch  139/139   train_loss = 3.213   time_elapsed = 130549.589   time_remaining = 2123\n",
      "Epoch 493 Batch  139/139   train_loss = 3.209   time_elapsed = 130808.508   time_remaining = 1857\n",
      "Epoch 494 Batch  139/139   train_loss = 3.199   time_elapsed = 131067.518   time_remaining = 1592\n",
      "Epoch 495 Batch  139/139   train_loss = 3.205   time_elapsed = 131327.191   time_remaining = 1327\n",
      "Epoch 496 Batch  139/139   train_loss = 3.199   time_elapsed = 131587.191   time_remaining = 1061\n",
      "Epoch 497 Batch  139/139   train_loss = 3.198   time_elapsed = 131847.191   time_remaining = 796\n",
      "Epoch 498 Batch  139/139   train_loss = 3.182   time_elapsed = 132107.217   time_remaining = 531\n",
      "Epoch 499 Batch  139/139   train_loss = 3.181   time_elapsed = 132367.250   time_remaining = 265\n",
      "Epoch 500 Batch  139/139   train_loss = 3.182   time_elapsed = 132627.187   time_remaining = 0\n"
     ]
    }
   ],
   "source": [
    "pickle.dump((hp.seq_length,  hp.save_dir), open('params.p', 'wb'))\n",
    "batches = get_batches(corpus_int, hp)\n",
    "num_batches = len(batches)\n",
    "start_time = time.time()\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    recover_last_checkpoints(hp.save_dir)\n",
    "    for epoch in range(hp.num_epochs):\n",
    "        state = sess.run(initial_state, {input_text: batches[0][0]})\n",
    "        \n",
    "        for batch_index, (x, y) in enumerate(batches):\n",
    "            feed_dict = {\n",
    "                input_text: x,\n",
    "                targets: y,\n",
    "                initial_state: state,\n",
    "                lr: hp.learning_rate\n",
    "            }\n",
    "            train_loss, state, _ = sess.run([cost, final_state, train_op], feed_dict)\n",
    "            \n",
    "        time_elapsed = time.time() - start_time\n",
    "        print('Epoch {:>3} Batch {:>4}/{}   train_loss = {:.3f}   time_elapsed = {:.3f}   time_remaining = {:.0f}'.format(\n",
    "            epoch + 1,\n",
    "            batch_index + 1,\n",
    "            len(batches),\n",
    "            train_loss,\n",
    "            time_elapsed,\n",
    "            ((num_batches * hp.num_epochs)/((epoch + 1) * (batch_index + 1))) * time_elapsed - time_elapsed))\n",
    "\n",
    "        # save model every 10 epochs\n",
    "        if epoch % 10 == 0:\n",
    "            saver = tf.train.Saver()\n",
    "            saver.save(sess, hp.save_dir)\n",
    "            print('Model Trained and Saved')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./save\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "import codecs\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "class HyperParameters:\n",
    "    def __init__(self):\n",
    "        self.num_epochs = 500\n",
    "        self.batch_size = 512\n",
    "        self.rnn_size = 512\n",
    "        self.num_layers = 3\n",
    "        self.keep_prob = 0.7\n",
    "        self.embed_dim = 512\n",
    "        self.seq_length = 30\n",
    "        self.learning_rate = 0.001\n",
    "        self.save_dir = './save'\n",
    "\n",
    "hp = HyperParameters()\n",
    "\n",
    "def pick_word(probabilities, int_to_vocab):\n",
    "    \"\"\"\n",
    "    Pick the next word with some randomness\n",
    "    :param probabilities: Probabilites of the next word\n",
    "    :param int_to_vocab: Dictionary of word ids as the keys and words as the values\n",
    "    :return: String of the predicted word\n",
    "    \"\"\"\n",
    "    return np.random.choice(list(int_to_vocab.values()), 1, p=probabilities)[0]\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "corpus_int, vocab_to_int, int_to_vocab, token_dict = pickle.load(open('preprocess.p', mode='rb'))\n",
    "seq_length, save_dir = pickle.load(open('params.p', mode='rb'))\n",
    "\n",
    "\n",
    "gen_length = 30\n",
    "prime_words = 'looking'\n",
    "\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load the saved model\n",
    "    loader = tf.train.import_meta_graph(hp.save_dir + '.meta')\n",
    "    loader.restore(sess, hp.save_dir)\n",
    "    \n",
    "    # Get tensors from loaded graph\n",
    "    input_text = loaded_graph.get_tensor_by_name('input:0')\n",
    "    initial_state = loaded_graph.get_tensor_by_name('initial_state:0')\n",
    "    final_state = loaded_graph.get_tensor_by_name('final_state:0')\n",
    "    probs = loaded_graph.get_tensor_by_name('probs:0')\n",
    "    \n",
    "    # Sentences generation setup\n",
    "    gen_sentences = prime_words.split()\n",
    "    prev_state = sess.run(initial_state, {input_text: np.array([[1 for word in gen_sentences]])})\n",
    "    \n",
    "    # Generate sentences\n",
    "    for n in range(gen_length):\n",
    "        # Dynamic Input\n",
    "        dyn_input = [[vocab_to_int[word] for word in gen_sentences[-hp.seq_length:]]]\n",
    "        dyn_seq_length = len(dyn_input[0])\n",
    "\n",
    "        # Get Prediction\n",
    "        probabilities, prev_state = sess.run(\n",
    "            [probs, final_state],\n",
    "            {input_text: dyn_input, initial_state: prev_state})\n",
    "        \n",
    "        p = np.array(probabilities[0][dyn_seq_length-1,:])\n",
    "        p /= p.sum()\n",
    "        #print(\"Probabilities: \",sum(probabilities[0][dyn_seq_length-1,:]))\n",
    "        #print(\"Normalized P : \", sum(p))\n",
    "        pred_word = pick_word(p, int_to_vocab)\n",
    "\n",
    "        gen_sentences.append(pred_word)\n",
    "        \n",
    "    # Remove tokens\n",
    "    chapter_text = ' '.join(gen_sentences)\n",
    "    for key, token in token_dict.items():\n",
    "        chapter_text = chapter_text.replace(' ' + token.lower(), key)\n",
    "        \n",
    "    print(chapter_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
